{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "source": [
    "import requests\r\n",
    "import re\r\n",
    "import urllib\r\n",
    "import pandas as pd\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from time import sleep\r\n",
    "from random import randint\r\n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "source": [
    "def get_url(title, location):\r\n",
    "    template_url = 'https://www.indeed.com/jobs?q={}&l={}&fromage=14'\r\n",
    "    url = template_url.format(urllib.parse.quote(title), urllib.parse.quote(location))\r\n",
    "    return url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "source": [
    "get_url('Data scientist', 'United states')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'https://www.indeed.com/jobs?q=Data%20scientist&l=United%20states&fromage=14'"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "source": [
    "def get_records(card):\r\n",
    "\r\n",
    "    try:\r\n",
    "        job_title = card.find('h2', {'class':'jobTitle'}).findAll('span')[-1].text\r\n",
    "    except:\r\n",
    "        job_title = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_conpany = card.find('span', {'class':'companyName'}).text\r\n",
    "    except:\r\n",
    "        job_conpany = ''\r\n",
    "\r\n",
    "    try:\r\n",
    "        job_rating = card.find('span', {'class':'ratingsDisplay withRatingLink'}).text\r\n",
    "    except:\r\n",
    "        job_rating = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_location = [i.text for i in card.find_all('div', {'class':'companyLocation'})]\r\n",
    "    except:\r\n",
    "        job_location = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_salary =  card.find('span', {'class':'salary-snippet'}).text\r\n",
    "    except:\r\n",
    "        job_salary = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_date = card.find('span', {'class':'date'}).text\r\n",
    "    except:\r\n",
    "        job_date = ''\r\n",
    "\r\n",
    "    extract_date = datetime.today().strftime('%Y-%m-%d')\r\n",
    "\r\n",
    "    job_url = 'https://www.indeed.com'+ card['href']\r\n",
    "    return job_title, job_conpany, job_rating, job_location, job_salary, job_date, extract_date, job_url\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "source": [
    "def get_detail(url):\r\n",
    "    res = requests.get(url)\r\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\r\n",
    "\r\n",
    "    #job detail\r\n",
    "    info = {}\r\n",
    "    try:\r\n",
    "        job_detail = soup.find_all('div', {'class':'jobsearch-JobDescriptionSection-sectionItem'})\r\n",
    "        for detail in job_detail:\r\n",
    "            if detail.find('span'):\r\n",
    "                info[detail.find_all('div')[0].text] = [i.text for i in detail.find_all('span')]\r\n",
    "            else:\r\n",
    "                info[detail.find_all('div')[0].text] = [i.text for i in detail.find_all('div')[1:]]\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    \r\n",
    "    #job qualification\r\n",
    "    try:\r\n",
    "        jobQual = soup.find('div', {'id':'qualificationsSection'})\r\n",
    "        title = jobQual.find('h2').text\r\n",
    "        info[title] = [i.text for i in jobQual.find_all('li', {'class':'icl-u-xs-p--none jobsearch-ReqAndQualSection-item'})]\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "    #job description\r\n",
    "    try:\r\n",
    "        jobdesc = soup.find('div', {'id':'jobDescriptionText'})\r\n",
    "        info['Description'] = jobdesc.format_string\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "        \r\n",
    "    res.close()\r\n",
    "    return info\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "source": [
    "def get_info(title, location):\r\n",
    "    records = []\r\n",
    "    details = []\r\n",
    "    no_of_jobs = {}\r\n",
    "    url = get_url(title, location)\r\n",
    "    for i in range(1):\r\n",
    "        if i > 35:\r\n",
    "            wait = randint(30, 50)\r\n",
    "            sleep(wait)\r\n",
    "        print(url)\r\n",
    "        res = requests.get(url)\r\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\r\n",
    "\r\n",
    "        #get total jobs\r\n",
    "        no_of_job_string = soup.find(id='searchCountPages').text.strip()\r\n",
    "        no_of_job = int(re.sub('\\W+', '', no_of_job_string.split()[3]))\r\n",
    "        no_of_jobs[title] = no_of_job\r\n",
    "        try:\r\n",
    "            cards = soup.find(id='mosaic-provider-jobcards').findChildren(\"a\" , recursive=False)\r\n",
    "        except:\r\n",
    "            continue\r\n",
    "\r\n",
    "        for card in cards:\r\n",
    "            record = get_records(card)\r\n",
    "            records.append(record)\r\n",
    "            wait = randint(5, 12)\r\n",
    "            sleep(wait)\r\n",
    "            detail = get_detail(record[-1])\r\n",
    "            details.append(detail)\r\n",
    "        \r\n",
    "        try:\r\n",
    "            url = 'https://www.indeed.com'+soup.find('a', {'aria-label':\"Next\"})['href']\r\n",
    "            wait = randint(1, 10)\r\n",
    "            sleep(wait)\r\n",
    "        except:\r\n",
    "            break\r\n",
    "        res.close()\r\n",
    "    return records, details, no_of_jobs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "source": [
    "def saveDate(title, records, details):\r\n",
    "    col = ['Title', 'Company', 'Rating', 'Location', 'Salary', 'Post_date', 'Extract_date', 'Job_url']\r\n",
    "    df_basic = pd.DataFrame(records)\r\n",
    "    df_basic.columns = col\r\n",
    "    df_detail = pd.DataFrame.from_dict(details)\r\n",
    "    df = pd.concat([df_basic, df_detail], axis=1)\r\n",
    "    df.to_csv(f'{title}.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "source": [
    "no_of_jobs = {}\r\n",
    "\r\n",
    "#get records from 'Data scientist' in 'UniedStates'\r\n",
    "records, details, total = get_info('Data scientist', 'United states')\r\n",
    "saveDate('Data scientist', records, details)\r\n",
    "no_of_jobs['Data scientist'] = total\r\n",
    "\r\n",
    "sleep(300)\r\n",
    "#get records from 'Software programmer' in 'UniedStates'\r\n",
    "records, details, total = get_info('Software programmer', 'United states')\r\n",
    "saveDate('Software programmer', records, details)\r\n",
    "no_of_jobs['Software programmer'] = total\r\n",
    "\r\n",
    "sleep(300)\r\n",
    "#get records from 'Web programmer' in 'UniedStates'\r\n",
    "records, details, total = get_info('Web programme', 'United states')\r\n",
    "saveDate('Web programmer', records, details)\r\n",
    "no_of_jobs['Web programmer'] = total\r\n",
    "\r\n",
    "sleep(300)\r\n",
    "#get records from 'Information architect' in 'UniedStates'\r\n",
    "records, details, total = get_info('Information architect', 'United states')\r\n",
    "saveDate('Information architect', records, details)\r\n",
    "no_of_jobs['Information architect'] = total"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.indeed.com/jobs?q=Data%20scientist&l=United%20states&fromage=14\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'text'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3388/3885426690.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#get records from 'Data scientist' in 'UniedStates'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data scientist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'United states'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0msaveDate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Data scientist'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdetails\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mno_of_jobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Data scientist'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_3388/1719681239.py\u001b[0m in \u001b[0;36mget_info\u001b[1;34m(title, location)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[1;31m#get total jobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mno_of_job_string\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'searchCountPages'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mno_of_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\W+'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mno_of_job_string\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mno_of_jobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mno_of_job\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'text'"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "c5562ae4cb73c005d4d512b63371f9759c2bba9f1255843d95070cdaa5df3d5e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}