{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import requests\r\n",
    "import re\r\n",
    "import urllib\r\n",
    "import pandas as pd\r\n",
    "from bs4 import BeautifulSoup\r\n",
    "from time import sleep\r\n",
    "from random import randint\r\n",
    "from datetime import datetime"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_url(title, location, page):\r\n",
    "    template_url = 'https://www.indeed.com/jobs?q={}&l={}&fromage=14&start={}'\r\n",
    "    url = template_url.format(urllib.parse.quote(title), urllib.parse.quote(location), page)\r\n",
    "    return url"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def get_records(card):\r\n",
    "\r\n",
    "    try:\r\n",
    "        job_title = card.find('h2', {'class':'jobTitle'}).findAll('span')[-1].text\r\n",
    "    except:\r\n",
    "        job_title = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_conpany = card.find('span', {'class':'companyName'}).text\r\n",
    "    except:\r\n",
    "        job_conpany = ''\r\n",
    "\r\n",
    "    try:\r\n",
    "        job_rating = card.find('span', {'class':'ratingsDisplay withRatingLink'}).text\r\n",
    "    except:\r\n",
    "        job_rating = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_location = [i.text for i in card.find_all('div', {'class':'companyLocation'})]\r\n",
    "    except:\r\n",
    "        job_location = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_salary =  card.find('span', {'class':'salary-snippet'}).text\r\n",
    "    except:\r\n",
    "        job_salary = ''\r\n",
    "    \r\n",
    "    try:\r\n",
    "        job_date = card.find('span', {'class':'date'}).text\r\n",
    "    except:\r\n",
    "        job_date = ''\r\n",
    "\r\n",
    "    extract_date = datetime.today().strftime('%Y-%m-%d')\r\n",
    "\r\n",
    "    job_url = 'https://www.indeed.com'+ card['href']\r\n",
    "    return job_title, job_conpany, job_rating, job_location, job_salary, job_date, extract_date, job_url\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def get_detail(url):\r\n",
    "    res = requests.get(url)\r\n",
    "    soup = BeautifulSoup(res.text, 'html.parser')\r\n",
    "\r\n",
    "    #job detail\r\n",
    "    info = {}\r\n",
    "    try:\r\n",
    "        job_detail = soup.find_all('div', {'class':'jobsearch-JobDescriptionSection-sectionItem'})\r\n",
    "        for detail in job_detail:\r\n",
    "            if detail.find('span'):\r\n",
    "                info[detail.find_all('div')[0].text] = [i.text for i in detail.find_all('span')]\r\n",
    "            else:\r\n",
    "                info[detail.find_all('div')[0].text] = [i.text for i in detail.find_all('div')[1:]]\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "    \r\n",
    "    #job qualification\r\n",
    "    try:\r\n",
    "        jobQual = soup.find('div', {'id':'qualificationsSection'})\r\n",
    "        title = jobQual.find('h2').text\r\n",
    "        info[title] = [i.text for i in jobQual.find_all('li', {'class':'icl-u-xs-p--none jobsearch-ReqAndQualSection-item'})]\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "\r\n",
    "    #job description\r\n",
    "    try:\r\n",
    "        jobdesc = soup.find('div', {'id':'jobDescriptionText'})\r\n",
    "        info['Description'] = jobdesc.format_string\r\n",
    "    except:\r\n",
    "        pass\r\n",
    "        \r\n",
    "    res.close()\r\n",
    "    return info\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def get_info(title, location):\r\n",
    "    header = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/93.0.4577.63 Safari/537.36\"}\r\n",
    "    records = []\r\n",
    "    details = []\r\n",
    "    no_of_page = 0\r\n",
    "    for i in range(0, 600, 10):\r\n",
    "        url = get_url(title, location, i)\r\n",
    "        print(url)\r\n",
    "        res = requests.get(url, header)\r\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\r\n",
    "\r\n",
    "        #get total jobs\r\n",
    "        if no_of_page == 0:\r\n",
    "            try:\r\n",
    "                no_of_page_string = soup.find(id='searchCountPages').text.strip()\r\n",
    "                no_of_page = int(re.sub('\\W+', '', no_of_page_string.split()[3]))\r\n",
    "            except:\r\n",
    "                print('No total number')\r\n",
    "                pass\r\n",
    "\r\n",
    "        try:\r\n",
    "            cards = soup.find(id='mosaic-provider-jobcards').findChildren(\"a\" , recursive=False)\r\n",
    "        except:\r\n",
    "            print('No job list')\r\n",
    "            break\r\n",
    "\r\n",
    "        for card in cards:\r\n",
    "            record = get_records(card)\r\n",
    "            records.append(record)\r\n",
    "            wait = randint(5, 12)\r\n",
    "            sleep(wait)\r\n",
    "            detail = get_detail(record[-1])\r\n",
    "            details.append(detail)\r\n",
    "\r\n",
    "        wait = randint(30, 60)\r\n",
    "        sleep(wait)\r\n",
    "\r\n",
    "        # res.close()\r\n",
    "    return records, details, no_of_page*15"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def saveDate(title, records, details):\r\n",
    "    col = ['Title', 'Company', 'Rating', 'Location', 'Salary', 'Post_date', 'Extract_date', 'Job_url']\r\n",
    "    df_basic = pd.DataFrame(records)\r\n",
    "    df_basic.columns = col\r\n",
    "    df_detail = pd.DataFrame.from_dict(details)\r\n",
    "    df = pd.concat([df_basic, df_detail], axis=1)\r\n",
    "    df.to_csv(f'{title}.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "no_of_jobs = {}\r\n",
    "\r\n",
    "#get records from 'Data scientist' in 'UniedStates'\r\n",
    "# records, details, total = get_info('Data scientist', 'United states')\r\n",
    "# try:\r\n",
    "#     saveDate(f'Data scientist', records, details)\r\n",
    "# except:\r\n",
    "#     print(\"No data stored\")\r\n",
    "# no_of_jobs['Data scientist'] = total\r\n",
    "\r\n",
    "# sleep(300)\r\n",
    "# #get records from 'Software programmer' in 'UniedStates'\r\n",
    "# records, details, total = get_info('Software programmer', 'United states')\r\n",
    "# try:\r\n",
    "#     saveDate(f'Software programmer', records, details)\r\n",
    "# except:\r\n",
    "#     print(\"No data stored\")\r\n",
    "# no_of_jobs['Software programmer'] = total\r\n",
    "\r\n",
    "# sleep(300)\r\n",
    "# #get records from 'Web programmer' in 'UniedStates'\r\n",
    "records, details, total = get_info('Web programmer', 'United states')\r\n",
    "try:\r\n",
    "    saveDate(f'Web programmer', records, details)\r\n",
    "except:\r\n",
    "    print(\"No data stored\")\r\n",
    "no_of_jobs['Web programmer'] = total\r\n",
    "\r\n",
    "# sleep(300)\r\n",
    "# #get records from 'Information architect' in 'UniedStates'\r\n",
    "# records, details, total = get_info('Information architect', 'United states')\r\n",
    "# saveDate('Information architect', records, details)\r\n",
    "# no_of_jobs['Information architect'] = total"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "https://www.indeed.com/jobs?q=Web%20programmer&l=United%20states&fromage=14&start=0\n",
      "No total number\n",
      "No job list\n",
      "No data stored\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "no_of_jobs"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Software programmer': 930780}"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "len(records)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "889"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.2",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "interpreter": {
   "hash": "c5562ae4cb73c005d4d512b63371f9759c2bba9f1255843d95070cdaa5df3d5e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}